episode--------------------  0
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryfollower.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryfollower.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/agentleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/agentfollower.pth
Model loaded!
/home/j-zhong/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:739: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:775.)
  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/algorithm/DPPO.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(follower_action).detach().to(self.device))
Episode:  0
---------------------------training!
agent_type_list ['agent', 'adversary']
{'reward': 0.0, 'relative_reward': defaultdict(<class 'float'>, {'adversary_0': 0.0, 'adversary_1': 0.0, 'agent_0': 0.0}), 'agents all reward': 0.0, 'adversaries all reward': 0.0, 'a_loss agent leader': 0, 'a_loss agent follower': 0, 'c_loss agent leader': 0, 'c_loss agent follower': 0, 'a_loss adversary leader': 0, 'a_loss adversary follower': 0, 'c_loss adversary leader': 0, 'c_loss adversary follower': 0}
send over!!!!!!!!!!!!!!
episode--------------------  1
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryfollower.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryfollower.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/agentleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/agentfollower.pth
Model loaded!
Episode:  1
---------------------------training!
agent_type_list ['agent', 'adversary']
{'reward': -9.325514301439206, 'relative_reward': defaultdict(<class 'float'>, {'adversary_0': 0.0, 'adversary_1': 0.0, 'agent_0': -154.17653574940056}), 'agents all reward': -154.17653574940056, 'adversaries all reward': 0.0, 'a_loss agent leader': 0, 'a_loss agent follower': 0, 'c_loss agent leader': 0, 'c_loss agent follower': 0, 'a_loss adversary leader': 0, 'a_loss adversary follower': 0, 'c_loss adversary leader': 0, 'c_loss adversary follower': 0}
send over!!!!!!!!!!!!!!
episode--------------------  2
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryfollower.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/adversaryfollower.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/agentleader.pth
Model loaded!
Actor path: /home/j-zhong/work_place/Bi-Level-Actor-Critic-with-F/model/agentfollower.pth
